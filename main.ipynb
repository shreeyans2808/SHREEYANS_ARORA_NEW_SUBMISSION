{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/datasetsenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/datasetsenv/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "docs = load_dataset('irds/beir_hotpotqa', 'docs')\n",
    "queries = load_dataset('irds/beir_hotpotqa', 'queries')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load two embedding models\n",
    "def load_model(model_name):\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "small_model, small_tokenizer = load_model('bert-base-uncased')\n",
    "large_model, large_tokenizer = load_model('bert-large-uncased')\n",
    "\n",
    "def get_embeddings(texts, model, tokenizer, max_length=512):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Generating embeddings\"):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :].numpy())  # Use [CLS] token embedding\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def get_top_k_passages(query, model, tokenizer, doc_embeddings, docs, k=10):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    query_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    \n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "\n",
    "    similarities = np.dot(doc_embeddings, query_embedding.T).flatten()\n",
    "    top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    top_k_passages = [docs[i] for i in top_k_indices]\n",
    "    return top_k_passages\n",
    "\n",
    "\n",
    "# Main execution\n",
    "def main(docs, queries):\n",
    "\n",
    "    doc_texts = docs['text']\n",
    "    query_texts = queries['text']\n",
    "\n",
    "    print(\"Generating embeddings...\")\n",
    "    small_doc_embeddings = get_embeddings(doc_texts, small_model, small_tokenizer)\n",
    "    large_doc_embeddings = get_embeddings(doc_texts, large_model, large_tokenizer)\n",
    "\n",
    "    small_doc_embeddings = small_doc_embeddings / np.linalg.norm(small_doc_embeddings, axis=1, keepdims=True)\n",
    "    large_doc_embeddings = large_doc_embeddings / np.linalg.norm(large_doc_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "main(docs, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_ranking_model(model_name):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "small_rank_model, small_rank_tokenizer = load_ranking_model('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "\n",
    "large_rank_model, large_rank_tokenizer = load_ranking_model('nvidia/nv-rerankqa-mistral-4b-v3')\n",
    "\n",
    "def rerank_passages(query, passages, model, tokenizer, batch_size=8):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(0, len(passages), batch_size):\n",
    "        batch = passages[i:i+batch_size]\n",
    "        inputs = tokenizer([query] * len(batch), batch, truncation=True, padding=True, return_tensors=\"pt\", max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        batch_scores = outputs.logits.squeeze(-1).tolist()\n",
    "        scores.extend(batch_scores)\n",
    "    \n",
    "    ranked_passages = sorted(zip(passages, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [passage for passage, score in ranked_passages], [score for passage, score in ranked_passages]\n",
    "\n",
    "def rerank_with_multiple_models(query, passages, models_and_tokenizers, weights=None):\n",
    "    if weights is None:\n",
    "        weights = [1] * len(models_and_tokenizers)\n",
    "    \n",
    "    all_scores = []\n",
    "    for (model, tokenizer), weight in zip(models_and_tokenizers, weights):\n",
    "        _, scores = rerank_passages(query, passages, model, tokenizer)\n",
    "        all_scores.append([score * weight for score in scores])\n",
    "    \n",
    "    combined_scores = [sum(scores) for scores in zip(*all_scores)]\n",
    "    ranked_passages = sorted(zip(passages, combined_scores), key=lambda x: x[1], reverse=True)\n",
    "    return [passage for passage, score in ranked_passages], [score for passage, score in ranked_passages]\n",
    "\n",
    "def main1(docs, queries):\n",
    "\n",
    "    models_and_tokenizers = [\n",
    "        (small_rank_model, small_rank_tokenizer),\n",
    "        (large_rank_model, large_rank_tokenizer)\n",
    "    ]\n",
    "    \n",
    "    example_query = queries[0]['text']\n",
    "    initial_passages = get_top_k_passages(example_query, small_model, small_tokenizer, small_doc_embeddings, docs, k=20)\n",
    "    reranked_passages, scores = rerank_with_multiple_models(example_query, initial_passages, models_and_tokenizers)\n",
    "\n",
    "\n",
    "main1(docs, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import ndcg_score\n",
    "from typing import List, Dict\n",
    "\n",
    "def calculate_ndcg(relevance_scores: List[float], k: int = 10) -> float:\n",
    "    return ndcg_score([relevance_scores], [list(range(len(relevance_scores)))], k=k)\n",
    "\n",
    "def get_top_k_passages(query, model, tokenizer, doc_embeddings, docs, k=10):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    query_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    \n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "    similarities = np.dot(doc_embeddings, query_embedding.T).flatten()\n",
    "    top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    top_k_passages = [docs[i] for i in top_k_indices]\n",
    "    return top_k_passages, similarities[top_k_indices]\n",
    "\n",
    "def evaluate_retrieval(queries: Dict, docs: Dict, model, tokenizer, doc_embeddings, k: int = 10):\n",
    "    ndcg_scores = []\n",
    "    for query in tqdm(queries['text'], desc=\"Evaluating queries\"):\n",
    "        top_k_passages, similarities = get_top_k_passages(query, model, tokenizer, doc_embeddings, docs['text'], k)\n",
    "        relevance_scores = [queries['relevance'].get((query, passage), 0) for passage in top_k_passages]\n",
    "        ndcg_scores.append(calculate_ndcg(relevance_scores, k))\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "def simple_ranking_model(query: str, passages: List[str]) -> List[float]:\n",
    "    query_words = set(query.lower().split())\n",
    "    scores = []\n",
    "    for passage in passages:\n",
    "        passage_words = set(passage.lower().split())\n",
    "        overlap = len(query_words.intersection(passage_words))\n",
    "        scores.append(overlap)\n",
    "    return scores\n",
    "\n",
    "def evaluate_with_ranking(queries: Dict, docs: Dict, model, tokenizer, doc_embeddings, ranking_model, k: int = 10):\n",
    "    ndcg_scores = []\n",
    "    for query in tqdm(queries['text'], desc=\"Evaluating queries with ranking\"):\n",
    "        top_k_passages, _ = get_top_k_passages(query, model, tokenizer, doc_embeddings, docs['text'], k)\n",
    "        ranking_scores = ranking_model(query, top_k_passages)\n",
    "        reranked_indices = np.argsort(ranking_scores)[::-1]\n",
    "        reranked_passages = [top_k_passages[i] for i in reranked_indices]\n",
    "        relevance_scores = [queries['relevance'].get((query, passage), 0) for passage in reranked_passages]\n",
    "        ndcg_scores.append(calculate_ndcg(relevance_scores, k))\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "# Evaluation code\n",
    "print(\"Evaluating retrieval performance without ranking:\")\n",
    "small_ndcg = evaluate_retrieval(queries, docs, small_model, small_tokenizer, small_doc_embeddings)\n",
    "print(f\"NDCG@10 for BERT-base: {small_ndcg:.4f}\")\n",
    "large_ndcg = evaluate_retrieval(queries, docs, large_model, large_tokenizer, large_doc_embeddings)\n",
    "print(f\"NDCG@10 for BERT-large: {large_ndcg:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating retrieval performance with ranking:\")\n",
    "small_ndcg_ranked = evaluate_with_ranking(queries, docs, small_model, small_tokenizer, small_doc_embeddings, simple_ranking_model)\n",
    "print(f\"NDCG@10 for BERT-base with ranking: {small_ndcg_ranked:.4f}\")\n",
    "large_ndcg_ranked = evaluate_with_ranking(queries, docs, large_model, large_tokenizer, large_doc_embeddings, simple_ranking_model)\n",
    "print(f\"NDCG@10 for BERT-large with ranking: {large_ndcg_ranked:.4f}\")\n",
    "\n",
    "print(\"\\nAnalyzing impact of different embedding and ranking model combinations:\")\n",
    "combinations = [\n",
    "    (\"BERT-base\", \"No ranking\", small_ndcg),\n",
    "    (\"BERT-large\", \"No ranking\", large_ndcg),\n",
    "    (\"BERT-base\", \"Simple ranking\", small_ndcg_ranked),\n",
    "    (\"BERT-large\", \"Simple ranking\", large_ndcg_ranked)\n",
    "]\n",
    "\n",
    "for emb_model, ranking_model, score in combinations:\n",
    "    print(f\"{emb_model} with {ranking_model}: NDCG@10 = {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
